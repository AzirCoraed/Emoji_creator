{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a9879d-87a6-4cc2-83a4-a7cb33077288",
   "metadata": {},
   "source": [
    "# 基于深度学习创建自定义表情符号项目详解\n",
    "\n",
    "## 一、项目背景\n",
    "在当今数字化交流日益频繁的时代，表情符号（Emojis）已成为在线聊天、产品评论、品牌情感表达等众多场景中不可或缺的一部分。它能够传达非语言线索，丰富交流内容。随着计算机视觉和深度学习技术的飞速发展，从图像中检测人类情感并将其与表情符号建立联系成为了可能。本项目旨在利用深度学习技术，构建一个能够将人类面部表情分类，并映射到相应表情符号的系统。\n",
    "\n",
    "## 二、数据集介绍\n",
    "本项目使用的是FER2013（Facial Expression Recognition 2013）数据集。\n",
    "1. **数据规格**：该数据集由48×48像素的灰度面部图像组成。这些图像均经过处理，使其居中且在图像中占据相等的空间。\n",
    "2. **情感类别**：共包含7种面部情感类别，分别通过数字进行标识：\n",
    "    - 0：愤怒（angry）\n",
    "    - 1：厌恶（disgust）\n",
    "    - 2：恐惧（fear）\n",
    "    - 3：快乐（happy）\n",
    "    - 4：悲伤（sad）\n",
    "    - 5：惊讶（surprise）\n",
    "    - 6：自然（natural）\n",
    "\n",
    "可通过提供的链接下载该数据集：[Facial Expression Recognition Dataset](下载链接)\n",
    "\n",
    "项目相关链接：\n",
    "- https://www.kaggle.com/datasets/msambare/fer2013?resource=download\n",
    "\n",
    "- https://data-flair.training/blogs/create-emoji-with-deep-learning/\n",
    "## 三、项目实现步骤\n",
    "\n",
    "### （一）面部表情识别（使用CNN）\n",
    "1. **下载与准备数据集**\n",
    "    - 从上述指定链接下载FER2013数据集。\n",
    "    - 在本地创建一个名为`data`的文件夹，将下载后的数据集解压到该文件夹中，并确保在`data`文件夹下分别创建`train`和`test`两个子目录，用于存放训练数据和测试数据。\n",
    "\n",
    "2. **创建`train.py`文件并编写代码**\n",
    "    - **导入必要的库**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "172c0501-deb0-41f0-94b8-d8d008e3f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c912b8-fbb4-4ee0-9e39-87da06ec5fc8",
   "metadata": {},
   "source": [
    "\n",
    "- `numpy`：用于处理数值计算和数组操作。\n",
    "- `cv2`：OpenCV库，用于计算机视觉任务，如读取图像、图像处理等。\n",
    "- `Sequential`：Keras中的序贯模型，用于按顺序堆叠神经网络层。\n",
    "- `Dense`：全连接层，用于神经网络中的计算。\n",
    "- `Dropout`：用于防止过拟合，在训练过程中随机丢弃一些神经元。\n",
    "- `Flatten`：将多维数据展平为一维数据，以便输入到全连接层。\n",
    "- `Conv2D`：二维卷积层，用于提取图像特征。\n",
    "- `Adam`：一种优化器，用于更新神经网络的权重。\n",
    "- `MaxPooling2D`：二维最大池化层，用于下采样，减少数据量和计算量。\n",
    "- `ImageDataGenerator`：用于数据增强和生成训练数据和验证数据的生成器。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb70e0-f308-425b-bc6b-45a3d688c3ab",
   "metadata": {},
   "source": [
    "- **初始化训练和验证数据生成器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f6791e1-0bc6-47ac-acf4-986183670c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# 初始化数据生成器\n",
    "train_dir = 'C:/Users/admin/Desktop/project/Emoji/archive/train'\n",
    "val_dir = 'C:/Users/admin/Desktop/project/Emoji/archive/test'\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical'\n",
    ")\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=64,\n",
    "    color_mode=\"grayscale\",\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3eef45-b503-4b8d-91d1-1cbac36b02c9",
   "metadata": {},
   "source": [
    "- `train_dir`和`val_dir`分别指定训练数据和验证数据的目录。\n",
    "- `train_datagen`和`val_datagen`是`ImageDataGenerator`的实例，`rescale=1./255`表示将图像像素值归一化到0 - 1之间，因为原始图像像素值范围是0 - 255。\n",
    "- `train_generator`和`validation_generator`通过`flow_from_directory`方法从指定目录中读取图像数据，`target_size=(48,48)`将图像调整为48×48像素大小，`batch_size=64`表示每次从数据集中读取64个样本作为一个批次进行训练，`color_mode=\"grayscale\"`指定图像为灰度图，`class_mode='categorical'`表示标签采用独热编码（one - hot encoding）形式。\n",
    "\n",
    "- **构建卷积神经网络架构**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29da102d-6356-441d-ba17-978f834468fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "emotion_model = Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)))\n",
    "emotion_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57404570-18a6-4e61-9aac-09e350c903e7",
   "metadata": {},
   "source": [
    "- 模型的第一层是`Conv2D`层，有32个卷积核，卷积核大小为(3, 3)，激活函数使用`relu`，`input_shape=(48,48,1)`表示输入图像的形状为48×48像素的单通道灰度图。\n",
    "- 接着是多个`Conv2D`层和`MaxPooling2D`层的组合。`Conv2D`层用于提取图像的特征，随着层数的增加，卷积核的数量逐渐增多（如从32到64再到128），以提取更复杂的特征。`MaxPooling2D`层用于下采样，通过池化操作（这里池化大小为(2, 2)），将特征图的尺寸减半，减少计算量和参数数量，同时保留主要特征。\n",
    "- `Dropout`层用于防止过拟合，在训练过程中随机丢弃一定比例（如0.25或0.5）的神经元，使模型学习到更鲁棒的特征。\n",
    "- `Flatten`层将多维的特征图展平为一维向量，以便输入到全连接层。\n",
    "- 全连接层`Dense`，第一个`Dense`层有1024个神经元，激活函数为`relu`；最后一个`Dense`层有7个神经元，对应7种情感类别，激活函数为`softmax`，用于输出每个类别的概率。\n",
    "\n",
    "- **编译和训练模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5243b08-9070-460a-b6e0-63a1f3c3155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 219ms/step - accuracy: 0.2458 - loss: 1.8322 - val_accuracy: 0.3188 - val_loss: 1.7348\n",
      "Epoch 2/50\n",
      "\u001b[1m  1/448\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 220ms/step - accuracy: 0.2344 - loss: 1.8257"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.2344 - loss: 1.8257 - val_accuracy: 0.3237 - val_loss: 1.7365\n",
      "Epoch 3/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 220ms/step - accuracy: 0.3324 - loss: 1.6857 - val_accuracy: 0.4171 - val_loss: 1.5513\n",
      "Epoch 4/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.3906 - loss: 1.4768 - val_accuracy: 0.4156 - val_loss: 1.5511\n",
      "Epoch 5/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 244ms/step - accuracy: 0.4057 - loss: 1.5494 - val_accuracy: 0.4431 - val_loss: 1.4762\n",
      "Epoch 6/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 14ms/step - accuracy: 0.4062 - loss: 1.5000 - val_accuracy: 0.4460 - val_loss: 1.4744\n",
      "Epoch 7/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 221ms/step - accuracy: 0.4309 - loss: 1.4787 - val_accuracy: 0.4708 - val_loss: 1.3979\n",
      "Epoch 8/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.4688 - loss: 1.4014 - val_accuracy: 0.4746 - val_loss: 1.4013\n",
      "Epoch 9/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 199ms/step - accuracy: 0.4666 - loss: 1.4029 - val_accuracy: 0.4925 - val_loss: 1.3431\n",
      "Epoch 10/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6719 - loss: 1.0825 - val_accuracy: 0.4915 - val_loss: 1.3440\n",
      "Epoch 11/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 201ms/step - accuracy: 0.4926 - loss: 1.3365 - val_accuracy: 0.5066 - val_loss: 1.3113\n",
      "Epoch 12/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5781 - loss: 1.1222 - val_accuracy: 0.5063 - val_loss: 1.3123\n",
      "Epoch 13/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 200ms/step - accuracy: 0.5026 - loss: 1.3065 - val_accuracy: 0.5250 - val_loss: 1.2670\n",
      "Epoch 14/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5781 - loss: 1.1312 - val_accuracy: 0.5248 - val_loss: 1.2661\n",
      "Epoch 15/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 200ms/step - accuracy: 0.5230 - loss: 1.2563 - val_accuracy: 0.5331 - val_loss: 1.2377\n",
      "Epoch 16/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5781 - loss: 1.2253 - val_accuracy: 0.5326 - val_loss: 1.2368\n",
      "Epoch 17/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 200ms/step - accuracy: 0.5380 - loss: 1.2263 - val_accuracy: 0.5364 - val_loss: 1.2203\n",
      "Epoch 18/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5156 - loss: 1.2592 - val_accuracy: 0.5356 - val_loss: 1.2210\n",
      "Epoch 19/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 200ms/step - accuracy: 0.5496 - loss: 1.1916 - val_accuracy: 0.5409 - val_loss: 1.2026\n",
      "Epoch 20/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5312 - loss: 1.2186 - val_accuracy: 0.5395 - val_loss: 1.2059\n",
      "Epoch 21/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 202ms/step - accuracy: 0.5612 - loss: 1.1642 - val_accuracy: 0.5449 - val_loss: 1.1966\n",
      "Epoch 22/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6562 - loss: 1.0028 - val_accuracy: 0.5442 - val_loss: 1.1982\n",
      "Epoch 23/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 201ms/step - accuracy: 0.5664 - loss: 1.1533 - val_accuracy: 0.5565 - val_loss: 1.1614\n",
      "Epoch 24/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5469 - loss: 1.1219 - val_accuracy: 0.5566 - val_loss: 1.1645\n",
      "Epoch 25/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 203ms/step - accuracy: 0.5795 - loss: 1.1216 - val_accuracy: 0.5635 - val_loss: 1.1575\n",
      "Epoch 26/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.6562 - loss: 1.0567 - val_accuracy: 0.5649 - val_loss: 1.1576\n",
      "Epoch 27/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 203ms/step - accuracy: 0.5951 - loss: 1.0925 - val_accuracy: 0.5664 - val_loss: 1.1354\n",
      "Epoch 28/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.5625 - loss: 1.1125 - val_accuracy: 0.5668 - val_loss: 1.1355\n",
      "Epoch 29/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 202ms/step - accuracy: 0.6067 - loss: 1.0621 - val_accuracy: 0.5732 - val_loss: 1.1236\n",
      "Epoch 30/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.6875 - loss: 0.8624 - val_accuracy: 0.5716 - val_loss: 1.1240\n",
      "Epoch 31/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 205ms/step - accuracy: 0.6060 - loss: 1.0555 - val_accuracy: 0.5766 - val_loss: 1.1222\n",
      "Epoch 32/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.7188 - loss: 0.8634 - val_accuracy: 0.5766 - val_loss: 1.1219\n",
      "Epoch 33/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 203ms/step - accuracy: 0.6235 - loss: 1.0221 - val_accuracy: 0.5771 - val_loss: 1.1141\n",
      "Epoch 34/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.5312 - loss: 1.0215 - val_accuracy: 0.5762 - val_loss: 1.1129\n",
      "Epoch 35/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 260ms/step - accuracy: 0.6332 - loss: 0.9985 - val_accuracy: 0.5813 - val_loss: 1.1075\n",
      "Epoch 36/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.5156 - loss: 1.4239 - val_accuracy: 0.5826 - val_loss: 1.1054\n",
      "Epoch 37/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 295ms/step - accuracy: 0.6409 - loss: 0.9747 - val_accuracy: 0.5859 - val_loss: 1.0899\n",
      "Epoch 38/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - accuracy: 0.6719 - loss: 0.9065 - val_accuracy: 0.5859 - val_loss: 1.0902\n",
      "Epoch 39/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 274ms/step - accuracy: 0.6495 - loss: 0.9496 - val_accuracy: 0.5982 - val_loss: 1.0872\n",
      "Epoch 40/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.6406 - loss: 1.0383 - val_accuracy: 0.5971 - val_loss: 1.0877\n",
      "Epoch 41/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 264ms/step - accuracy: 0.6607 - loss: 0.9222 - val_accuracy: 0.5950 - val_loss: 1.0827\n",
      "Epoch 42/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.4844 - loss: 1.1790 - val_accuracy: 0.5931 - val_loss: 1.0845\n",
      "Epoch 43/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 263ms/step - accuracy: 0.6649 - loss: 0.9037 - val_accuracy: 0.5984 - val_loss: 1.0803\n",
      "Epoch 44/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.6250 - loss: 1.1518 - val_accuracy: 0.5982 - val_loss: 1.0823\n",
      "Epoch 45/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 270ms/step - accuracy: 0.6714 - loss: 0.8862 - val_accuracy: 0.6004 - val_loss: 1.0806\n",
      "Epoch 46/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 15ms/step - accuracy: 0.7031 - loss: 0.9864 - val_accuracy: 0.6002 - val_loss: 1.0811\n",
      "Epoch 47/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 265ms/step - accuracy: 0.6771 - loss: 0.8760 - val_accuracy: 0.5961 - val_loss: 1.0933\n",
      "Epoch 48/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.6719 - loss: 0.7764 - val_accuracy: 0.5949 - val_loss: 1.0986\n",
      "Epoch 49/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 264ms/step - accuracy: 0.6911 - loss: 0.8470 - val_accuracy: 0.6049 - val_loss: 1.0786\n",
      "Epoch 50/50\n",
      "\u001b[1m448/448\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - accuracy: 0.7188 - loss: 0.8053 - val_accuracy: 0.6057 - val_loss: 1.0797\n"
     ]
    }
   ],
   "source": [
    "# 训练模型，使用 fit 方法\n",
    "emotion_model_info = emotion_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=28709 // 64,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=7178 // 64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fbfa60-bdb8-4d6e-9c97-775d65a45dab",
   "metadata": {},
   "source": [
    "- `emotion_model.compile`用于编译模型，`loss='categorical_crossentropy'`表示使用分类交叉熵损失函数，适用于多分类问题；`optimizer=Adam(lr=0.0001, decay=1e-6)`使用Adam优化器，设置初始学习率为0.0001，学习率衰减为`1e - 6`；`metrics=['accuracy']`表示在训练过程中监控模型的准确率。\n",
    "- `emotion_model.fit_generator`用于训练模型，`train_generator`为训练数据生成器，`steps_per_epoch=28709 // 64`表示每个epoch需要训练的步数，`28709`是训练数据集中样本的总数，除以`batch_size=64`得到步数；`epochs=50`表示训练50个epoch；`validation_data=validation_generator`指定验证数据生成器，`validation_steps=7178 // 64`表示每个epoch验证时的步数，`7178`是验证数据集中样本的总数。\n",
    "\n",
    "- **保存模型权重**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cb05479-e0f1-41c0-8a8c-7862928b5ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.save_weights('model.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8135d-f41c-4555-87f5-0b283f05a336",
   "metadata": {},
   "source": [
    "将训练好的模型权重保存为`model.h5`文件，以便后续使用。\n",
    "\n",
    "- **使用OpenCV进行人脸检测和情感预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf280718-c4d2-445b-8e3d-aeb543cd50cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv2.ocl.setUseOpenCL(False)\n",
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # 使用 cv2.data.haarcascades 动态获取 Haar 级联分类器文件的路径\n",
    "    bounding_box = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    num_faces = bounding_box.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y - 50), (x + w, y + h + 10), (255, 0, 0), 2)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "        cv2.putText(frame, emotion_dict[maxindex], (x + 20, y - 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    cv2.imshow('Video', cv2.resize(frame, (1200, 860), interpolation=cv2.INTER_CUBIC))\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73153e24-d10a-46bb-b0fd-8f946c1e129a",
   "metadata": {},
   "source": [
    "- `cv2.ocl.setUseOpenCL(False)`：禁用OpenCL加速，因为在某些情况下可能会导致问题。\n",
    "- `emotion_dict`是一个字典，用于将模型预测的数字标签映射到对应的情感类别字符串。\n",
    "- `cap = cv2.VideoCapture(0)`：打开默认的摄像头设备。\n",
    "- 在循环中，通过`cap.read()`读取摄像头的每一帧图像。如果读取失败（`not ret`），则退出循环。\n",
    "- 使用OpenCV的Haar级联分类器`cv2.CascadeClassifier`加载人脸检测模型（`haarcascade_frontalface_default.xml`），将彩色图像转换为灰度图像`gray_frame`，然后使用`detectMultiScale`方法检测灰度图像中的人脸，`scaleFactor=1.3`表示每次图像缩放的比例，`minNeighbors=5`表示每个候选矩形需要保留的邻居数，以确定是否为真正的人脸。\n",
    "- 对于检测到的每一个人脸，使用`cv2.rectangle`在原始图像上绘制矩形框标记人脸。然后提取人脸区域`roi_gray_frame`，对其进行缩放和维度扩展，使其符合模型输入的要求（形状为(1, 48, 48, 1)）。\n",
    "- 使用训练好的模型`emotion_model.predict`对提取的人脸图像进行情感预测，得到一个概率数组。通过`np.argmax`找到概率最大的索引，即预测的情感类别索引`maxindex`。\n",
    "- 使用`cv2.putText`在图像上显示预测的情感类别字符串。\n",
    "- 使用`cv2.imshow`显示处理后的图像，`cv2.waitKey(1)`等待按键事件，如果按下`q`键，则释放摄像头资源并关闭所有OpenCV窗口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9865d8f-6667-441e-b3c8-e6e8ff2ebba9",
   "metadata": {},
   "source": [
    "\n",
    "## 四、项目总结\n",
    "本项目通过构建卷积神经网络（CNN），在FER2013数据集上进行训练，实现了面部表情的识别，并将识别出的情感映射到相应的表情符号。利用OpenCV的Haar级联分类器进行人脸检测，同时通过`tkinter`创建了图形用户界面，方便用户直观地看到面部表情对应的表情符号。通过这个项目，初学者可以深入了解深度学习在计算机视觉领域的应用，包括数据集处理、模型构建、训练以及GUI开发等多个方面。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
